{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as etree\n",
    "import os\n",
    "import xlrd\n",
    "import xlwt\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = 'dataset tugas/Training set'\n",
    "training_text = []\n",
    "for filename in os.listdir(path):\n",
    "    if not filename.endswith('.xml'): continue\n",
    "    fullname = os.path.join(path, filename)\n",
    "    tree = etree.parse(fullname)\n",
    "    root = tree.getroot()\n",
    "    for element in root.iter(tag= 'headline'):\n",
    "        temp = []\n",
    "        temp.append(element.text)\n",
    "        for element in root.iter(tag = 'p'):\n",
    "            temp.append(element.text)\n",
    "        training_text.append(' '.join(temp))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path1 = 'dataset tugas/Testing set'\n",
    "testing_text = []\n",
    "for filename in os.listdir(path1):\n",
    "    if not filename.endswith('.xml'): continue\n",
    "    fullname = os.path.join(path1, filename)\n",
    "    tree = etree.parse(fullname)\n",
    "    root = tree.getroot()\n",
    "    for element in root.iter(tag= 'headline'):\n",
    "        temp = []\n",
    "        temp.append(element.text)\n",
    "        for element in root.iter(tag = 'p'):\n",
    "            temp.append(element.text)\n",
    "        testing_text.append(' '.join(temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cleaned_text(data):\n",
    "    new = []\n",
    "    for i in range(len(data)):\n",
    "        corpus = re.sub('[^a-zA-z]','',data[i])\n",
    "        corpus = corpus.lower()\n",
    "        corpus = corpus.split()\n",
    "        corpus = [stemmer.stem(word) for word in corpus if not word in stop_words]\n",
    "        corpus = ''.join(corpus)\n",
    "        new.append(corpus)\n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#def bag_of_word(corpus):\n",
    " #   cv = CountVectorizer()\n",
    "  #  tfidf =cv.fit_transform(corpus).toarray()\n",
    "   # return tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_NB(X,y):\n",
    "    klasifikasi = GaussianNB()\n",
    "    klasifikasi.fit(X,y)\n",
    "    pred = klasifikasi.predict(X)\n",
    "    \n",
    "    akurasi = metrics.accuracy_score(y, pred)*100\n",
    "    print(\"Akurasi Naive Bayes = \"+str(akurasi))\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_KNN(X,y):\n",
    "    klasifikasi = KNeighborsClassifier(n_neighbors=17, n_jobs=-1)\n",
    "    klasifikasi.fit(X,y)\n",
    "    pred = klasifikasi.predict(X)\n",
    "    \n",
    "    akurasi = metrics.accuracy_score(y, pred)*100\n",
    "    print(\"Akurasi Model KNN = \"+str(akurasi))\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def testing_NB(X,y, xtes, ytes):\n",
    "    klasifikasi = GaussianNB()\n",
    "    klasifikasi.fit(X,y)\n",
    "    pred = klasifikasi.predict(xtes)\n",
    "    presisiMNB = metrics.precision_score(ytes, pred, average='micro')\n",
    "    print(\"Presisi Naive Bayes: \", presisiMNB*100)\n",
    "    \n",
    "    recallMNB = metrics.recall_score(ytes, pred, average='micro')\n",
    "    print(\"Recall Naive Bayes: \", recallMNB*100)\n",
    "    \n",
    "    f1MNB = metrics.f1_score(ytes, pred, average='micro')\n",
    "    print(\"f1 Naive Bayes: \", f1MNB*100)\n",
    "    \n",
    "    akurasi = metrics.accuracy_score(ytes, pred)*100\n",
    "    print(\"Akurasi Naive Bayes = \"+str(akurasi))\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def testing_KNN(X,y, xtes, ytes):\n",
    "    klasifikasi = KNeighborsClassifier(n_neighbors=17, n_jobs=-1)\n",
    "    klasifikasi.fit(X,y)\n",
    "    pred = klasifikasi.predict(xtes)\n",
    "    \n",
    "    presisiKNN = metrics.precision_score(ytes, pred, average='micro')\n",
    "    print(\"Presisi KNN: \", presisiKNN*100)\n",
    "    \n",
    "    recallKNN = metrics.recall_score(ytes, pred, average='micro')\n",
    "    print(\"Recall KNN: \", recallKNN*100)\n",
    "    \n",
    "    f1KNN = metrics.f1_score(ytes, pred, average='micro')\n",
    "    print(\"f1 KNN: \", f1KNN*100)\n",
    "    \n",
    "    akurasi = metrics.accuracy_score(ytes, pred)*100\n",
    "    print(\"Akurasi KNN = \"+str(akurasi))\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned = cleaned_text(training_text)\n",
    "cleaned_test= cleaned_text(testing_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(cleaned_test)):\n",
    "    cleaned.append(cleaned_test[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "tokenized = sent_tokenize(str(cleaned))\n",
    "for i in tokenized:\n",
    "    wList = nltk.word_tokenize(i)\n",
    "    wList = [w for w in wList if not w in stop_words]\n",
    "    tagged = nltk.pos_tag(wList)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = bag_of_word(cleaned)\n",
    "X_training = []\n",
    "X_testing = []\n",
    "\n",
    "#for i in range(577):\n",
    "#    X_training.append(X[i])\n",
    "    \n",
    "#for i in range(577,600):\n",
    "   # X_testing.append(X[i])\n",
    "    \n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf = vectorizer.fit_transform(cleaned).toarray()\n",
    "X_training = tfidf[:577]\n",
    "X_testing = tfidf[577:]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "book = xlrd.open_workbook('dataset tugas/Training_set.xlsx')\n",
    "book_save = xlwt.Workbook()\n",
    "sheet = book.sheet_by_index(0)\n",
    "sheet_save = book_save.add_sheet(\"sheet1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Akurasi Naive Bayes = 100.0\n",
      "Akurasi Model KNN = 76.2564991334\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([' YES', ' YES', ' YES', ' YES', ' YES', ' YES', ' YES', ' YES',\n",
       "       ' YES', ' YES', ' YES', ' YES', ' YES', ' YES', ' YES', ' YES',\n",
       "       ' YES', ' YES', 'NO', 'NO', 'NO', 'NO', ' YES', ' YES', 'NO', 'NO',\n",
       "       'NO', 'NO', 'NO', ' YES', 'NO', 'NO', ' YES', ' YES', ' YES',\n",
       "       ' YES', ' YES', ' YES', ' YES', ' YES', ' YES', ' YES', ' YES',\n",
       "       ' YES', ' YES', ' YES', ' YES', ' YES', ' YES', ' YES', ' YES',\n",
       "       ' YES', ' YES', ' YES', ' YES', ' YES', ' YES', ' YES', ' YES',\n",
       "       ' YES', ' YES', ' YES', ' YES', ' YES', ' YES', ' YES', ' YES',\n",
       "       ' YES', ' YES', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO',\n",
       "       'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO',\n",
       "       'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO',\n",
       "       'NO', 'NO', 'NO', 'NO', 'NO', 'NO', ' YES', ' YES', ' YES', ' YES',\n",
       "       ' YES', ' YES', ' YES', ' YES', ' YES', ' YES', ' YES', ' YES',\n",
       "       ' YES', ' YES', 'NO', ' YES', ' YES', ' YES', ' YES', ' YES',\n",
       "       ' YES', ' YES', ' YES', ' YES', ' YES', ' YES', ' YES', ' YES',\n",
       "       ' YES', ' YES', ' YES', ' YES', ' YES', ' YES', ' YES', ' YES',\n",
       "       ' YES', ' YES', ' YES', ' YES', ' YES', ' YES', ' YES', ' YES',\n",
       "       ' YES', ' YES', ' YES', ' YES', ' YES', ' YES', ' YES', ' YES',\n",
       "       ' YES', ' YES', ' YES', ' YES', ' YES', ' YES', ' YES', ' YES',\n",
       "       ' YES', ' YES', ' YES', ' YES', ' YES', ' YES', ' YES', ' YES',\n",
       "       ' YES', ' YES', ' YES', ' YES', ' YES', ' YES', ' YES', ' YES',\n",
       "       ' YES', ' YES', ' YES', ' YES', ' YES', ' YES', ' YES', ' YES',\n",
       "       ' YES', ' YES', ' YES', ' YES', ' YES', ' YES', ' YES', ' YES',\n",
       "       ' YES', ' YES', ' YES', ' YES', ' YES', ' YES', ' YES', ' YES',\n",
       "       ' YES', ' YES', ' YES', ' YES', ' YES', ' YES', ' YES', ' YES',\n",
       "       ' YES', ' YES', ' YES', ' YES', ' YES', ' YES', ' YES', ' YES',\n",
       "       ' YES', ' YES', ' YES', ' YES', ' YES', ' YES', ' YES', ' YES',\n",
       "       ' YES', ' YES', ' YES', ' YES', ' YES', ' YES', ' YES', ' YES',\n",
       "       ' YES', ' YES', ' YES', ' YES', ' YES', ' YES', ' YES', ' YES',\n",
       "       ' YES', ' YES', ' YES', ' YES', ' YES', ' YES', ' YES', ' YES',\n",
       "       ' YES', ' YES', ' YES', ' YES', ' YES', ' YES', ' YES', ' YES',\n",
       "       ' YES', ' YES', ' YES', ' YES', ' YES', ' YES', ' YES', ' YES',\n",
       "       ' YES', ' YES', ' YES', ' YES', ' YES', ' YES', ' YES', ' YES',\n",
       "       ' YES', ' YES', ' YES', ' YES', ' YES', ' YES', ' YES', ' YES',\n",
       "       ' YES', ' YES', ' YES', ' YES', ' YES', ' YES', ' YES', ' YES',\n",
       "       ' YES', ' YES', ' YES', ' YES', ' YES', ' YES', ' YES', ' YES',\n",
       "       ' YES', ' YES', ' YES', ' NO', ' YES', ' YES', ' YES', ' NO', ' NO',\n",
       "       ' NO', ' NO', ' YES', ' YES', ' YES', ' NO', ' YES', ' NO', ' NO',\n",
       "       ' NO', ' NO', ' YES', ' YES', ' YES', ' YES', ' YES', ' YES',\n",
       "       ' YES', ' YES', ' YES', ' YES', ' YES', ' YES', ' YES', ' YES',\n",
       "       ' YES', ' YES', ' YES', ' YES', ' YES', ' YES', ' YES', ' YES',\n",
       "       ' YES', ' YES', ' YES', ' YES', ' YES', ' YES', ' YES', ' YES',\n",
       "       ' YES', ' YES', ' YES', ' YES', ' YES', ' YES', ' YES', ' YES',\n",
       "       ' YES', ' YES', ' YES', ' YES', ' YES', ' YES', ' YES', ' YES',\n",
       "       ' YES', ' YES', ' YES', ' YES', ' YES', ' YES', ' YES', ' YES',\n",
       "       ' YES', ' YES', ' YES', ' YES', ' YES', ' YES', ' YES', ' YES',\n",
       "       ' YES', ' YES', ' YES', ' YES', ' YES', ' YES', 'NO', ' YES',\n",
       "       ' YES', ' YES', ' YES', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO',\n",
       "       'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO',\n",
       "       'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO',\n",
       "       'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO', ' YES', 'NO', 'NO', 'NO',\n",
       "       'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO',\n",
       "       'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO',\n",
       "       'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO',\n",
       "       'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO',\n",
       "       'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO',\n",
       "       'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO',\n",
       "       'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO',\n",
       "       'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO',\n",
       "       'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO',\n",
       "       'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO',\n",
       "       'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO',\n",
       "       'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO',\n",
       "       'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO', ' YES', ' YES'],\n",
       "      dtype='<U4')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_training = []\n",
    "for i in range(0,577):\n",
    "    label_training.append(sheet.cell_value(i,1))\n",
    "    \n",
    "y = label_training\n",
    "\n",
    "train_NB(X_training,y)\n",
    "train_KNN(X_training,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "book = xlrd.open_workbook('dataset tugas/Testing_set.xlsx')\n",
    "book_save = xlwt.Workbook()\n",
    "sheet = book.sheet_by_index(0)\n",
    "\n",
    "\n",
    "label_testing = []\n",
    "for i in range(0,23):\n",
    "    label_testing.append(sheet.cell_value(i,1))\n",
    "    \n",
    "y_testing = label_testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Presisi Naive Bayes:  69.5652173913\n",
      "Recall Naive Bayes:  69.5652173913\n",
      "f1 Naive Bayes:  69.5652173913\n",
      "Akurasi Naive Bayes = 69.5652173913\n",
      "Presisi KNN:  30.4347826087\n",
      "Recall KNN:  30.4347826087\n",
      "f1 KNN:  30.4347826087\n",
      "Akurasi KNN = 30.4347826087\n"
     ]
    }
   ],
   "source": [
    "prendiksi_NB = testing_NB(X_training,label_training, X_testing, y_testing)\n",
    "prendiksi_KNN = testing_KNN(X_training,label_training, X_testing, y_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
